{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hairy-graphic",
   "metadata": {},
   "source": [
    "## Data Peprocessing (word segmentation, keyword extracton, word normalisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satisfied-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-orleans",
   "metadata": {},
   "source": [
    "### Load CKIP (download data if you haven't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment line below. Downloads to ./data.zip (2GB) and extracts to ./data/\n",
    "### data_utils.download_data_gdown(\"./\") # gdrive-ckip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "progressive-grass",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/melody/Documents/Courses 2021 autumn/Data Mining/labs/lib/python3.9/site-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/Users/melody/Documents/Courses 2021 autumn/Data Mining/labs/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:984: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._kernel = self.add_variable(\n",
      "/Users/melody/Documents/Courses 2021 autumn/Data Mining/labs/lib/python3.9/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:993: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  self._bias = self.add_variable(\n",
      "2021-12-04 17:54:58.266508: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/melody/Documents/Courses 2021 autumn/Data Mining/labs/lib/python3.9/site-packages/ckiptagger/model_pos.py:56: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
      "/Users/melody/Documents/Courses 2021 autumn/Data Mining/labs/lib/python3.9/site-packages/ckiptagger/model_ner.py:57: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n"
     ]
    }
   ],
   "source": [
    "from ckiptagger import data_utils, construct_dictionary, WS, POS, NER\n",
    "import tensorflow\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "ws = WS(\"./data\", disable_cuda=False)\n",
    "pos = POS(\"./data\", disable_cuda=False)\n",
    "ner = NER(\"./data\", disable_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-pittsburgh",
   "metadata": {},
   "source": [
    "#### Preprocessed keywords\n",
    "... or just use the preprocessed files (skip 3 cells below, find+use those with `.jsonl` extension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dated-inspiration",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "keyword_pth = \"../Keywords\"\n",
    "kw_files = defaultdict(lambda: list())\n",
    "\n",
    "for filename in os.listdir(keyword_pth):\n",
    "    print(filename)\n",
    "    if filename.split(\".\")[-1] == \"xlsx\":\n",
    "        continue\n",
    "    with open(os.path.join(keyword_pth, filename), newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "        for row in spamreader:\n",
    "            kw_files[filename].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acknowledged-powder",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kw_files_cleaned = defaultdict(lambda: list())\n",
    "\n",
    "for filename, file_cont in kw_files.items():\n",
    "    for name_lst in file_cont:  \n",
    "        new_names = []\n",
    "        for name in name_lst:\n",
    "            if name.strip():\n",
    "                new_names.append(name)\n",
    "        kw_files_cleaned[filename].append(new_names.copy())\n",
    "        \n",
    "kw_files_cleaned['02chem.list.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "crazy-community",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for filename in kw_files_cleaned.keys():\n",
    "    file_newname = \".\".join(filename.split(\".\")[:-1])\n",
    "    file_newname = file_newname+\".jsonl\"\n",
    "    with jsonlines.open(file_newname, mode='w') as writer:\n",
    "        writer.write(kw_files_cleaned[filename])     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-kazakhstan",
   "metadata": {},
   "source": [
    "<< **Load keyword files here.** >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "potential-nowhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "\n",
    "kwords = []\n",
    "for filename in os.listdir(\"./Keywords\"):\n",
    "    if filename.split(\".\")[-1]==\"jsonl\":\n",
    "        kwords.extend([line for line in jsonlines.open(\"./Keywords/\"+filename)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "distributed-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "kw_all = []\n",
    "\n",
    "for type_ in kwords: # crop, pest, chem\n",
    "    for it in type_:\n",
    "        for i in it:\n",
    "            kw_all.append(i)\n",
    "kw_all = set(kw_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operational-baking",
   "metadata": {},
   "source": [
    "Make dictionaries for text normalisation:\n",
    "- `synonym_dict`: `{normalised keyword: [other keywords (to be replaced)]}`\n",
    "    - Just load `synonym_dict.json` 3 cells below for you convenience\n",
    "- `inv_synonym_dict`: inverted `synonym_dict` to perform the replacing (i.e. values become keys and vice versa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "greatest-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_dict = {}\n",
    "synonym_set = set()\n",
    "\n",
    "for filename, filecont in kw_files_cleaned.items():\n",
    "    for namelist in filecont:\n",
    "        if len(namelist) > 1:\n",
    "            synonym_dict[namelist[0]] = namelist[1:]\n",
    "            synonym_set  = synonym_set | set(namelist[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "serious-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('斑飛蝨', {'穗苔'})"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(synonym_dict.items())[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "searching-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"synonym_dict.json\", \"w\") as f:\n",
    "    json.dump(synonym_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-retreat",
   "metadata": {},
   "source": [
    "<< ** Load `synonym_dict` here ** >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "mysterious-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"synonym_dict.json\") as f:\n",
    "    synonym_dict = json.load(f)\n",
    "\n",
    "inv_synonym_dict = dict()\n",
    "\n",
    "for k, v1 in synonym_dict.items():\n",
    "    for v in v1: \n",
    "        inv_synonym_dict[v] = k\n",
    "    inv_synonym_dict[k] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-longer",
   "metadata": {},
   "source": [
    "### Create coerce & recommend dictionary from keywords for ckip \n",
    "(code from group member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "revised-curve",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Path = Path = './Keywords/'\n",
    "### create a coerce dictionary by the pest list and the chem list\n",
    "dfpest = pd.read_excel(Path+'02pest.list.xlsx', engine='openpyxl', header=None)\n",
    "dfchem = pd.read_excel(Path+'02chem.list.xlsx', engine='openpyxl', header=None)\n",
    "\n",
    "coerce_dict = pd.concat([dfpest, dfchem]).to_numpy().reshape(-1).tolist()\n",
    "coerce_dict = construct_dictionary(\n",
    "    dict((word, 1) for word in coerce_dict if pd.isnull(word) == False)\n",
    "    )\n",
    "\n",
    "### create a recommend dictionary only by the crop list\n",
    "dfcrop = pd.read_excel(Path+'02crop.list.xlsx', engine='openpyxl', header=None)\n",
    "recommend_dict = dfcrop.to_numpy().reshape(-1).tolist()\n",
    "recommend_dict = construct_dictionary(\n",
    "    dict((word, 1) for word in recommend_dict if pd.isnull(word) == False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silent-failure",
   "metadata": {},
   "source": [
    "### Word segmentation + extract keywords for each article\n",
    "You may also just use the preprocessed file `doc_keywords.json` 3 cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-compilation",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "Path = './dataTrainComplete/'\n",
    "filelist = os.listdir(Path)\n",
    "doc_kw_dict = dict()\n",
    "\n",
    "for file in tqdm(filelist):\n",
    "    if file[0]=='.':\n",
    "        continue\n",
    "    f = open(Path+file, 'r', encoding='utf-8')\n",
    "    article = \"\".join([line.strip() for line in f.readlines()])\n",
    "    article = re.sub('[0-9]+', \"N\", article)\n",
    "    article = re.sub('[a-zA-Z]+', \"A\", article)\n",
    "    article = re.sub('[^\\u4e00-\\u9fa5\\uff0c\\u3001\\u3002\\uff1bNA]+|\\u7684','', article)\n",
    "#     artilist.append(re.sub('[0-9]+|\\x7f|\\n|\\!|\\'|\\?|\\）|\\（|\\、|\\。','', f.read()))\n",
    "    seg_res = ws([article], recommend_dictionary=recommend_dict, coerce_dictionary=coerce_dict)  # 进行分词\n",
    "    seg_res = flattenList(seg_res)\n",
    "    kws = get_keywords(kw_all, seg_res)\n",
    "    \n",
    "    docnum = int(file.split(\".\")[0])\n",
    "    doc_kw_dict[docnum] = kws.copy()\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "thick-compression",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flattenList(nestedList):\n",
    "     # source:  geeks2geeks\n",
    "    # check if list is empty\n",
    "    if not(bool(nestedList)):\n",
    "        return nestedList\n",
    " \n",
    "     # to check instance of list is empty or not\n",
    "    # a list is an instance of list\n",
    "    # a str, int, etc., is not an instance of list, and will therefore be returned as the final line of this func\n",
    "    if isinstance(nestedList[0], list):\n",
    " \n",
    "        # call function with sublist as argument\n",
    "        return flattenList(*nestedList[:1]) + flattenList(nestedList[1:])\n",
    " \n",
    "    # call function with sublist as argument\n",
    "    return nestedList[:1] + flattenList(nestedList[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "significant-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(all_kw_set, seg_res):\n",
    "    \"\"\"\n",
    "    seg_res = segmentation results from ckip (output of ws)\n",
    "    \"\"\"\n",
    "    kw_lst = []\n",
    "    for w in seg_res:\n",
    "        if w in all_kw_set:\n",
    "            kw_lst.append(w)\n",
    "    return kw_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "extensive-buying",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"doc_keywords.json\", 'w') as f:\n",
    "    json.dump(doc_kws, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-consumption",
   "metadata": {},
   "source": [
    "<< **Load `doc_keywords` here** >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "identified-allen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# get keywords for each document\n",
    "with open(\"doc_keywords.json\") as f:\n",
    "    doc_kws_ = json.load(f)\n",
    "\n",
    "# turn keys from string to integer\n",
    "doc_kws = dict()\n",
    "for k in doc_kws_.keys():\n",
    "    doc_kws[int(k)] = doc_kws_[k].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-angola",
   "metadata": {},
   "source": [
    "### Build data structure for storing docs with keywords \n",
    "Prob not necessary in hindsight, built this mainly for the graph thing but used it anyway. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "efficient-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, node_num, kwords):\n",
    "        # when calling Node(in_nodes, out_nodes, kwords), __init__ will be executed\n",
    "        \"\"\"\n",
    "        node_num: document id\n",
    "        kwords: list of strings; represents the keywords that the node article contains\n",
    "        \"\"\"\n",
    "        self.node_num = node_num\n",
    "        self.kwords = kwords\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "viral-stopping",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nodes = dict()\n",
    "\n",
    "for doc in doc_id_sorted:\n",
    "    cur_kws = set()\n",
    "    for w in doc_kws[doc]:\n",
    "        if w in inv_synonym_dict.keys(): # 需替換\n",
    "            cur_kws.add(inv_synonym_dict[w])\n",
    "        else: # 為本名/無別名，不需替換\n",
    "            cur_kws.add(w)\n",
    "    node = Node(doc, cur_kws)\n",
    "    nodes[doc] = node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consecutive-keeping",
   "metadata": {},
   "source": [
    "---\n",
    "### ...finally the classification based on intersection of keyword sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "vocational-attitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1727"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on training data\n",
    "ans = dict()\n",
    "kws = []\n",
    "ratios = []\n",
    "intersec_ratio = 1\n",
    "kw_ratio = 0.4 # start with lower values e.g. 0.2 and scale up \n",
    "#lower_ratio = 0.4\n",
    "\n",
    "for doc in nodes.values():\n",
    "    for doc_cmp in nodes.values():\n",
    "        doc_kw = doc.kwords\n",
    "        cmp_kw = doc_cmp.kwords\n",
    "        intersec = doc_kw & cmp_kw\n",
    "        if len(doc_kw) and doc.node_num != doc_cmp.node_num:\n",
    "            intersec_ratio = len(intersec)/len(doc_kw)\n",
    "            if not bool(intersec):\n",
    "                continue\n",
    "            elif intersec_ratio >= intersec_ratio: # 兩文件的相似度\n",
    "                if len(doc_kw)/len(cmp_kw) > kw_ratio: # 被比較的文件的 keywords 在另一文件中的佔比\n",
    "                    ans_dict = {\"kws\": (doc_kw, cmp_kw), \"ratios\": intersec_ratio}\n",
    "                    ans_key = str(doc.node_num)+\"-\"+str(doc_cmp.node_num)\n",
    "                    ans[ans_key] = ans_dict\n",
    "                # if len(doc_kw) > len(cmp_kw) => (cmp, doc) will be in ans but not (doc, cmp)\n",
    "            #elif intersec_ratio > lower_ratio and intersec_ratio < intersec_ratio:\n",
    "                #ans.append((doc.node_num, doc_cmp.node_num))\n",
    "                #ans.append((doc_cmp.node_num, doc.node_num))\n",
    "len(ans.keys())          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-puppy",
   "metadata": {},
   "source": [
    "---\n",
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "upper-bread",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, gold):\n",
    "    \"\"\"\n",
    "    TP/(TP+FP)\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    for p in pred:\n",
    "        if p in gold:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    return tp/(tp+fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "western-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred, gold):\n",
    "    \"\"\"\n",
    "    TP/(TP+FN)\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fn = 0\n",
    "    for g in gold:\n",
    "        if g in pred:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn+=1\n",
    "    return tp/(tp+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "asian-inquiry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def F1(prec, rcl):\n",
    "    return 2*prec*rcl/(prec+rcl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cubic-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"TrainLabel.csv\"\n",
    "related_pairs = []\n",
    "\n",
    "with open(csv_path, newline='') as csvfile:\n",
    "    spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    rows = list(spamreader)\n",
    "    for it in rows[1:]: \n",
    "        related_pairs.append([int(it[0]), int(it[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "driven-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ans.keys()\n",
    "gold = [str(tp[0])+\"-\"+str(tp[1]) for tp in related_pairs]\n",
    "\n",
    "precision_score = precision(pred, gold)\n",
    "recall_score = recall(pred, gold)\n",
    "f1_score = F1(precision_score, recall_score)\n",
    "\n",
    "print(\"{:.2%}, {:.2%}, {:.2%}\".format(precision_score, recall_score, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "homeless-throw",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "better-occurrence",
   "metadata": {},
   "source": [
    "---\n",
    "### Read test data\n",
    "Or load directly from ready-made file (3 cells below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "cooperative-transformation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docs = dict()\n",
    "\n",
    "for filename in os.listdir(\"./dataTestPublicComplete\"):\n",
    "    if filename[0] == \".\":\n",
    "        continue\n",
    "    with open(\"./dataTestPublicComplete/\"+filename) as f:\n",
    "        test_docs[int(filename.split(\".\")[0])] = \"\".join([line.strip() for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "determined-bolivia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'近來氣候高溫多濕，相當適合草莓苗期炭疽病發生，籲請農友注意防治適期。苗栗區農改場、防檢局及田邊好幫手關心您。近來氣候高溫多濕，相當適合草莓苗期炭疽病發生，籲請農友注意防治適期。防治應注意事項:(1)請加強母株的防治，若育苗區母株已呈現萎凋狀，請徹底清除勿留在園區。(2)去除老葉之傷口為炭疽病菌侵入冠部之重要關鍵，老葉去除後應施用藥劑，以防病菌感染。育苗區之葉片、走蔓若有炭疽病斑，請將較嚴重之葉片或走蔓去除，並於24小時內配合藥劑之噴施。(3)噴施藥劑時須特別注意冠部亦應有足夠之劑量，因冠部若腐壞會造成植株萎凋死亡。農友如果有任何技術上的疑問，可以直接洽詢苗栗區農業改良場諮詢專線037-236583或病蟲害診斷專線037-236619，或各鄉鎮市公所、農會。防治藥劑及方法：(1)53％腐絕快得寧可濕性粉劑1,200倍(2)24.9%待克利乳劑3,000倍(3)24.9%待克利水懸劑3,000倍(4)23.6%百克敏乳劑3,000倍'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs[1053]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "roman-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_docs.json\", 'w') as f:\n",
    "    json.dump(test_docs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-launch",
   "metadata": {},
   "source": [
    "<< ** Load `test_docs` here ** >>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_docs.json\") as f:\n",
    "    test_docs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-cooper",
   "metadata": {},
   "source": [
    "### Preprocess keywords\n",
    "( or skip 1 cell and just use the ready-made file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "czech-mercy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 421/421 [04:17<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# segmentation for test docs\n",
    "test_doc_kw = dict()\n",
    "\n",
    "for docnum, article in tqdm(test_docs.items()):\n",
    "    article = re.sub('[0-9]+', \"N\", article)\n",
    "    article = re.sub('[a-zA-Z]+', \"A\", article)\n",
    "    article = re.sub('[^\\u4e00-\\u9fa5\\uff0c\\u3001\\u3002\\uff1bNA]+|\\u7684','', article)\n",
    "    #     artilist.append(re.sub('[0-9]+|\\x7f|\\n|\\!|\\'|\\?|\\）|\\（|\\、|\\。','', f.read()))\n",
    "    seg_res = ws([article], recommend_dictionary=recommend_dict, coerce_dictionary=coerce_dict)  # 进行分词\n",
    "    seg_res = flattenList(seg_res)\n",
    "    kws = get_keywords(kw_all, seg_res)\n",
    "    test_doc_kw[docnum] = kws.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "interim-extent",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_doc_kw.json\") as f:\n",
    "    test_doc_kws_ = json.load(f)\n",
    "\n",
    "test_doc_kws = dict()\n",
    "for k in test_doc_kws_.keys():\n",
    "    test_doc_kws[int(k)] = test_doc_kws_[k].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-translation",
   "metadata": {},
   "source": [
    "### Make data structure for storing doc and keyword info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "precise-palace",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_nodes = dict()\n",
    "\n",
    "for doc in test_docs.keys():\n",
    "    test_cur_kws = set()\n",
    "    for w in test_doc_kws[doc]:\n",
    "        if w in inv_synonym_dict.keys(): # 需替換\n",
    "            test_cur_kws.add(inv_synonym_dict[w])\n",
    "        else: # 為本名/無別名，不需替換\n",
    "            test_cur_kws.add(w)\n",
    "    test_node = Node(doc, test_cur_kws)\n",
    "    test_nodes[doc] = test_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "worldwide-batman",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'待克利', '炭疽病', '百克敏', '腐絕快得寧', '草莓'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_nodes[1053].kwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "neutral-thread",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "615"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test on training data\n",
    "test_ans = dict()\n",
    "test_kws = []\n",
    "test_ratios = []\n",
    "intersec_ratio = 1\n",
    "kw_ratio = 0.4\n",
    "#lower_ratio = 0.4\n",
    "no_intersec = dict() # for error analysis\n",
    "\n",
    "for doc in test_nodes.values():\n",
    "    for doc_cmp in test_nodes.values():\n",
    "        doc_kw = doc.kwords\n",
    "        cmp_kw = doc_cmp.kwords\n",
    "        intersec = doc_kw & cmp_kw\n",
    "        \n",
    "        if len(doc_kw) and doc.node_num != doc_cmp.node_num:\n",
    "            intersec_ratio = len(intersec)/len(doc_kw)\n",
    "            if not bool(intersec):\n",
    "                ni_dict = {\"kws\": (doc_kw, cmp_kw)}\n",
    "                ni_key = str(doc.node_num)+\"-\"+str(doc_cmp.node_num)\n",
    "                no_intersec[ni_key] = ni_dict\n",
    "                continue\n",
    "            elif intersec_ratio >= intersec_ratio: # 兩文件的相似度\n",
    "                if len(doc_kw) / len(cmp_kw) > kw_ratio:\n",
    "                    ans_dict = {\"kws\": (doc_kw, cmp_kw), \"ratios\": intersec_ratio}\n",
    "                    ans_key = str(doc.node_num)+\"-\"+str(doc_cmp.node_num)\n",
    "                    test_ans[ans_key] = ans_dict\n",
    "                # if len(doc_kw) > len(cmp_kw) => (cmp, doc) will be in ans but not (doc, cmp)\n",
    "            #elif intersec_ratio > lower_ratio and intersec_ratio < intersec_ratio:\n",
    "                #ans.append((doc.node_num, doc_cmp.node_num))\n",
    "                #ans.append((doc_cmp.node_num, doc.node_num))\n",
    "len(test_ans.keys())          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "crucial-production",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "functioning-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "test_ans_pairs = []\n",
    "for k in test_ans.keys():\n",
    "    test_ans_pairs.append(k.split(\"-\"))\n",
    "    \n",
    "submit_num = 5\n",
    "    \n",
    "with open('submit{}.csv'.format(submit_num), 'w', newline='') as f:\n",
    "    spamwriter = csv.writer(f, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([\"Test\",\"Reference\"])\n",
    "    for k in test_ans_pairs:\n",
    "        spamwriter.writerow(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
